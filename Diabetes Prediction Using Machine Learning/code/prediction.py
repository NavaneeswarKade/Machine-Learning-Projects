# -*- coding: utf-8 -*-
"""Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1paea5ErPqPmSklFotRO28PmQl90xRVs_
"""

# Importing essential libraries
import numpy as np
import pandas as pd
import pickle
import matplotlib.pyplot as plt

# Loading the dataset
df = pd.read_csv(r'diabetes.csv')

df.head()

df.info()

df.isnull()

df.sum()

df.describe()

import seaborn as sns
sns.countplot(x='Outcome', data=df)
plt.title('Level')
plt.show()

sns.boxplot(data = df,x = "Outcome", y = "BMI")

# Renaming DiabetesPedigreeFunction as DPF
df = df.rename(columns={'DiabetesPedigreeFunction':'DPF'})

plt.figure(figsize = (30,20))
sns.set(font_scale=1.5)
sns.heatmap(df.corr(),annot=True, cmap='GnBu')
plt.title("Diabetes Variable Correlations",fontsize=30)

# Replacing the 0 values from ['Glucose','BloodPressure','SkinThickness','Insulin','BMI'] by NaN
df_copy = df.copy(deep=True)
df_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df_copy[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)

# Replacing NaN value by mean, median depending upon distribution
df_copy['Glucose'].fillna(df_copy['Glucose'].mean(), inplace=True)
df_copy['BloodPressure'].fillna(df_copy['BloodPressure'].mean(), inplace=True)
df_copy['SkinThickness'].fillna(df_copy['SkinThickness'].median(), inplace=True)
df_copy['Insulin'].fillna(df_copy['Insulin'].median(), inplace=True)
df_copy['BMI'].fillna(df_copy['BMI'].median(), inplace=True)

# Model Building
from sklearn.model_selection import train_test_split
X = df.drop(columns='Outcome')
y = df['Outcome']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)

df.info()

X=df.drop('Outcome',axis=1)
Y=df['Outcome']

Y.value_counts()

from imblearn.under_sampling import NearMiss
nm = NearMiss(version = 1,n_neighbors=5)
x_sm,y_sm=nm.fit_resample(X,Y)

y_sm.shape , x_sm.shape

y_sm.value_counts()

X_train , X_test , Y_train , Y_test = train_test_split(x_sm,y_sm, test_size=0.3 , random_state=42)

for x in [X_train, X_test, Y_train, Y_test]:
    print(len(x))

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier


grid_models = [
    (KNeighborsClassifier(), [{'n_neighbors': [8, 10, 13], 'weights': ['uniform', 'distance'], 'algorithm': ['auto', 'ball_tree', 'brute']}]),
    (DecisionTreeClassifier(), [{'criterion': ['gini', 'entropy', 'log_loss'], 'min_samples_leaf': [4, 5, 6], 'max_depth': [8, 10, 13]}]),
    (RandomForestClassifier(), [{'n_estimators': [50, 100, 150, 200, 250], 'max_depth': [8, 10, 13], 'criterion': ['gini', 'entropy'], 'max_features': [1, 3, 5]}])
]

for i,j in grid_models:
    grid = GridSearchCV(estimator=i,param_grid = j, scoring = 'accuracy',cv=2)
    grid.fit(X_train, Y_train)
    best_accuracy = grid.best_score_
    best_param = grid.best_params_
    print('{}:\nBest Accuracy : {:.2f}%'.format(i,best_accuracy*100))
    print('Best Parameters : ',best_param)
    print('')
    print('----------------')
    print('')

"""# KNN"""

knn = KNeighborsClassifier(algorithm='auto',n_neighbors=13, weights='uniform')
knn.fit(X_train , Y_train)

# make predictions on test set
y_pred=knn.predict(X_test)

print('Training set score: {:.4f}'.format(knn.score(X_train, Y_train)))

print('Test set score: {:.4f}'.format(knn.score(X_test, Y_test)))

from sklearn.metrics import mean_squared_error
import math  # Import math module if not already imported

# Assuming Y_test and y_pred are defined
mse = mean_squared_error(Y_test, y_pred)
print('Mean Squared Error: ' + str(mse))

rmse = math.sqrt(mse)
print('Root Mean Squared Error: ' + str(rmse))

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
matrix = classification_report(Y_test,y_pred )
print(matrix)

import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

# Assuming Y_test and y_pred are defined
plot_confusion_matrix(Y_test, y_pred)

"""# Decision Tree"""

dt = DecisionTreeClassifier(criterion='gini',max_depth=13,min_samples_leaf=5)
dt.fit(X_train,Y_train)

# Make predictions on test data
y_pred=dt.predict(X_test)
print('Training set score: {:.4f}'.format(dt.score(X_train,Y_train)))

print('Test set score: {:.4f}'.format(dt.score(X_train,Y_train)))

# Check MSE and RSME
mse=mean_squared_error(Y_test,y_pred)
print('Mean Squared Error : '+str(mse))

rmse=math.sqrt(mse)
print('Mean Squared Error :'+str(rmse))

# Create Decision Tree Classification Report
matrix = classification_report(Y_test,y_pred )
print(matrix)

import seaborn as sns
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt

def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

# Assuming Y_test and y_pred are defined
plot_confusion_matrix(Y_test, y_pred)

"""# Random Forest"""

rf = RandomForestClassifier(max_depth=13, criterion='gini', n_estimators =200, min_samples_leaf=5, random_state=42)
rf.fit(X_train, Y_train)

# Make predictions on test set
y_pred=rf.predict(X_test)

print('Training set score: {:.4f}'.format(rf.score(X_train, Y_train)))

print('Test set score: {:.4f}'.format(rf.score(X_test, Y_test)))

# Check MSE & RMSE
mse =mean_squared_error(Y_test, y_pred)
print('Mean Squared Error : '+ str(mse))
rmse = math.sqrt(mean_squared_error(Y_test, y_pred))
print('Root Mean Squared Error : '+ str(rmse))

matrix = classification_report(Y_test,y_pred )
print(matrix)

def plot_confusion_matrix(y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=True)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

# Assuming Y_test and y_pred are defined
plot_confusion_matrix(Y_test, y_pred)

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

pipeline_lr = Pipeline([('scaler', StandardScaler()),
                        ('lr_classifier', KNeighborsClassifier())])

pipeline_dt = Pipeline([('scaler', StandardScaler()),
                        ('dt_classifier', DecisionTreeClassifier())])

pipeline_rf = Pipeline([('scaler', StandardScaler()),
                        ('rf_classifier', RandomForestClassifier())])

# List of pipelines for ease of iteration
pipelines = [pipeline_lr, pipeline_dt, pipeline_rf]

# Dictionary of pipelines and classifier types for ease of reference
pipe_dict = {0: 'Logistic Regression', 1: 'Decision Tree', 2: 'Random Forest'}

# Fit the pipelines
for pipe in pipelines:
    pipe.fit(X_train, Y_train)

# Compare accuracies
for i,model in enumerate(pipelines):
    print("{} Test Accuracy: {}".format(pipe_dict[i],model.score(X_test,Y_test)))

"""#Model Building"""

# Creating Random Forest Model
from sklearn.ensemble import RandomForestClassifier
classifier = RandomForestClassifier(n_estimators=20)
classifier.fit(X_train, Y_train)

# Creating a pickle file for the classifier
filename = 'Prediction.pkl'
pickle.dump(classifier, open(filename, 'wb'))